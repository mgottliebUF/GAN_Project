#DOGGG

import matplotlib.pyplot as plt
import numpy as np
import torch
from torch import nn, optim
import torch.nn.functional as func
from torch.utils.data import Dataset, DataLoader
from torchvision import datasets, transforms
from torchvision.utils import save_image, make_grid
import os
import torchvision
import xml.etree.ElementTree as ET
from tqdm import tqdm_notebook as tqdm
import time
import torch.nn.utils.spectral_norm as SN

# Set the device to GPU if available, otherwise use CPU
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# Define paths for images and annotations
IMG_PATH = '../input/generative-dog-images/all-dogs/'
ANN_PATH = "../input/generative-dog-images/annotation/Annotation/"
OUT_PATH = '../output_images'
IMG_DIR = "output_images/"

# Set various hyperparameters
IMG_SIZE = 64
BATCH_SZ = 32
PRNT_INTRVL = 1000
SHOW_INTRVL = 1000
PLOT_INTRVL = 200
NUM_EPOCHS = 100

# Set learning rates and beta values for Adam optimizer
d_lr = 0.001
g_lr = 0.001
beta1_val = 0.5
beta2_val = 0.999

# Set dimensions and feature sizes for the GAN
latent_dim = 128
feature_size = 512
spectral_norm = True
normalization_type = 'adain'
use_random_noise = True
apply_style = True

# Define loss type and normalization options
loss_type = 'HINGE'
pixel_norm = True

# Define label options
use_soft_labels = True
invert_labels = True
true_label = 0.9
false_label = 0

# Set the number of images to be generated
num_images = 10000
compute_leadboard = True

# Define a custom dataset class for loading dog images
class DogImagesDataset(Dataset):
    def __init__(self, root_dir, trans1=None, trans2=None):
        self.root_dir = root_dir
        self.trans1 = trans1
        self.trans2 = trans2
        self.imgs, self.crop_coords = self._load_images(self.root_dir)
        if len(self.imgs) == 0:
            raise RuntimeError("No files found in subfolders of: {}".format(self.root_dir))
            
    def _load_images(self, root_dir):
        IMG_EXT = ('.jpg', '.jpeg', '.png', '.ppm', '.bmp', '.pgm', '.tif', '.tiff', '.webp')
        def is_valid_img(x):
            return torchvision.datasets.folder.has_file_allowed_extension(x, IMG_EXT)

        img_files = []
        crop_boxes = []
        paths = []
        for root, _, fnames in sorted(os.walk(root_dir)):
            for fname in sorted(fnames):
                path = os.path.join(root, fname)
                paths.append(path)

        pbar = tqdm(paths, desc='Loading cropped images')
        for path in pbar:
            if is_valid_img(path):
                img = torchvision.datasets.folder.default_loader(path)
                ann_basename = os.path.splitext(os.path.basename(path))[0]
                ann_dirname = next(dirname for dirname in os.listdir(ANN_PATH) if dirname.startswith(ann_basename.split('_')[0]))
                ann_filename = os.path.join(ANN_PATH, ann_dirname, ann_basename)
                tree = ET.parse(ann_filename)
                root = tree.getroot()
                size = root.find('size')
                width = int(size.find('width').text)
                height = int(size.find('height').text)
                objects = root.findall('object')
                for obj in objects:    
                    bndbox = obj.find('bndbox') 
                    xmin = int(bndbox.find('xmin').text)
                    ymin = int(bndbox.find('ymin').text)
                    xmax = int(bndbox.find('xmax').text)
                    ymax = int(bndbox.find('ymax').text)
                    xmin = max(0, xmin - 4)
                    xmax = min(width, xmax + 4)
                    ymin = max(0, ymin - 4)
                    ymax = min(height, ymax + 4)
                    crop_width = np.min((xmax - xmin, ymax - ymin))
                    crop_width = min(crop_width, width, height)
                    if crop_width > xmax - xmin:
                        xmin = min(max(0, xmin - int((crop_width - (xmax - xmin)) / 2)), width - crop_width)
                        xmax = xmin + crop_width
                    if crop_width > ymax - ymin:
                        ymin = min(max(0, ymin - int((crop_width - (ymax - ymin)) / 2)), height - crop_width)
                        ymax = ymin + crop_width
                    cropped_img = img.crop((xmin, ymin, xmax, ymax))
                    if np.mean(cropped_img) != 0:
                        img_files.append(path)
                        crop_boxes.append((xmin, ymin, xmax, ymax))
                pbar.set_postfix_str("{} cropped images loaded".format(len(img_files)))

        return img_files, crop_boxes
    
    def __getitem__(self, index):
        img_path = self.imgs[index]
        img = torchvision.datasets.folder.default_loader(img_path)
        img = img.crop(self.crop_coords[index])
        if self.trans2 is not None:
            img = self.trans2(img)
        return img

    def __len__(self):
        return len(self.imgs)

# Define transformations for data augmentation
transformation1 = None
transformation2 = transforms.Compose([
    transforms.RandomResizedCrop(64, (0.85, 1.0), (1.0, 1.0)),
    transforms.ColorJitter(),
    transforms.RandomHorizontalFlip(p=0.5),
    transforms.ToTensor(),
    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))
])

# Create dataset and data loader
dataset = DogImagesDataset(
    IMG_PATH,
    transformation1,
    transformation2
)
data_loader = torch.utils.data.DataLoader(dataset, shuffle=True,
                                          batch_size=BATCH_SZ, num_workers=4)

# Display some training images
batch_imgs = next(iter(data_loader))
plt.figure(figsize=(8, 8))
plt.axis("off")
plt.title("Training Images")
plt.imshow(np.transpose(make_grid(batch_imgs.to(device)[:64], padding=2, normalize=True).cpu(), (1, 2, 0)))

# Define a custom normalization module
class NormModulate2d(nn.Module):
    def __init__(self, num_feats, in_dim, epsilon=2e-5, momentum=0.1, affine=True, 
                 track_running_stats=True, apply_sn=True):
        super().__init__()
        self.num_feats = num_feats
        self.in_dim = in_dim
        self.norm = nn.BatchNorm2d(num_feats, affine=False)
        self.gamma = nn.Sequential(
            nn.Linear(in_dim, num_feats, bias=True),
            nn.LeakyReLU(0.2),
            nn.Linear(num_feats, num_feats, bias=False)
        )
        self.beta = nn.Sequential(
            nn.Linear(in_dim, num_feats, bias=True),
            nn.LeakyReLU(0.2),
            nn.Linear(num_feats, num_feats, bias=False)
        )

    def forward(self, x, z):
        out = self.norm(x)
        gamma = self.gamma(z)
        beta = self.beta(z)
        out = gamma.view(-1, self.num_feats, 1, 1) * out + beta.view(-1, self.num_feats, 1, 1)
        return out

# Define a pixel normalization layer
class PixelNormLayer(nn.Module):
    def __init__(self, epsilon=1e-8):
        super(PixelNormLayer, self).__init__()
        self.epsilon = epsilon

    def forward(self, x):
        temp = torch.mul(x, x)
        temp1 = torch.rsqrt(torch.mean(temp, dim=1, keepdim=True) + self.epsilon)
        return x * temp1

# Define a Gaussian noise layer
class GaussianNoiseLayer(nn.Module):
    def __init__(self, channels):
        super().__init__()
        self.weight = nn.Parameter(torch.zeros(1, channels, 1, 1))

    def forward(self, x, noise=None):
        if noise is None:
            noise = torch.randn(x.shape[0], 1, x.shape[2], x.shape[3], device=x.device, dtype=x.dtype)
        return x + self.weight.view(1, -1, 1, 1) * noise.to(x.device)

# Define an adaptive instance normalization layer
class AdaptiveInstanceNorm(nn.Module):
    def __init__(self, in_ch, latent_dim):
        super().__init__()
        self.norm = nn.InstanceNorm2d(in_ch)
        self.style = nn.Linear(latent_dim, in_ch * 2)

    def forward(self, input, style):
        style = self.style(style).unsqueeze(2).unsqueeze(3)
        gamma, beta = style.chunk(2, 1)
        out = self.norm(input)
        out = gamma * out + beta
        return out

# Define a custom projection layer
class ProjectLayer(nn.Module):
    def __init__(self, 
                 latent_dim, 
                 in_ch, 
                 out_ch, 
                 shape, 
                 bias=False, 
                 apply_sn=False, 
                 norm_type='selfmod', 
                 random_noise=False,
                 use_style=False,
                 apply_pixel_norm=False):
        super().__init__()
        self.shape = shape
        self.linear = nn.Linear(in_ch, out_ch, bias=bias)
        self.conv = nn.Conv2d(shape[0], shape[0], 3, 1, 1, bias=bias)
        if apply_sn:
            self.linear = SN(self.linear)
            self.conv = SN(self.conv)
            
        self.noise1 = None
        if random_noise:
            self.noise1 = GaussianNoiseLayer(shape[0])
            self.noise2 = GaussianNoiseLayer(shape[0])
            
        self.pixel_norm = None
        if apply_pixel_norm:
            self.pixel_norm = PixelNormLayer()
            
        self.style1 = None
        self.style2 = None
            
        if norm_type == 'adain':
            self.norm1 = AdaptiveInstanceNorm(shape[0], latent_dim)
            self.norm2 = AdaptiveInstanceNorm(shape[0], latent_dim)
        else:
            self.norm1 = NormModulate2d(shape[0], latent_dim)
            self.norm2 = NormModulate2d(shape[0], latent_dim)
            
    def forward(self, x, latent_vec):
        x = self.linear(x)
        x = x.view([x.shape[0]] + self.shape)
        if self.noise1 is not None:
            x = self.noise1(x)
        x = func.leaky_relu(x, 0.2)
        if self.pixel_norm is not None:
            x = self.pixel_norm(x)
        x = self.norm1(x, latent_vec)
        x = self.conv(x)
        if self.noise2 is not None:
            x = self.noise2(x)
        x = func.leaky_relu(x, 0.2)
        if self.pixel_norm is not None:
            x = self.pixel_norm(x)
        x = self.norm2(x, latent_vec)
        return x

# Define an upconvolutional block layer
class UpConvLayer(nn.Module):
    def __init__(self, 
                 latent_dim, 
                 in_ch, 
                 out_ch, 
                 kernel_size=4, 
                 stride=2, 
                 padding=1, 
                 bias=False, 
                 apply_sn=False, 
                 norm_type='selfmod', 
                 random_noise=False,
                 use_style=False,
                 apply_pixel_norm=False):
        super().__init__()
        self.conv1 = nn.ConvTranspose2d(in_ch, out_ch, kernel_size, stride, padding, bias=bias)
        self.conv2 = nn.Conv2d(out_ch, out_ch, 3, 1, 1, bias=bias)
        if apply_sn:
            self.conv1 = SN(self.conv1)
            self.conv2 = SN(self.conv2)
            
        self.noise1 = None
        self.noise2 = None
        if random_noise:
            self.noise1 = GaussianNoiseLayer(out_ch)
            self.noise2 = GaussianNoiseLayer(out_ch)
            
        self.pixel_norm = None
        if apply_pixel_norm:
            self.pixel_norm = PixelNormLayer()
            
        self.style1 = None
        self.style2 = None
        
        if norm_type == 'adain':
            self.norm1 = AdaptiveInstanceNorm(out_ch, latent_dim)
            self.norm2 = AdaptiveInstanceNorm(out_ch, latent_dim)
        else:
            self.norm1 = NormModulate2d(out_ch, latent_dim)
            self.norm2 = NormModulate2d(out_ch, latent_dim)
            
    def forward(self, x, latent_vec):
        x = self.conv1(x)
        if self.noise1 is not None:
            x = self.noise1(x)
        x = func.leaky_relu(x, 0.2)
        if self.pixel_norm is not None:
            x = self.pixel_norm(x)
        x = self.norm1(x, latent_vec)
        x = self.conv2(x)
        if self.noise2 is not None:
            x = self.noise2(x)
        x = func.leaky_relu(x, 0.2)
        if self.pixel_norm is not None:
            x = self.pixel_norm(x)
        x = self.norm2(x, latent_vec)
        return x

# Define the generator model
class GenModel(nn.Module):
    def __init__(self, 
                 latent_dim, 
                 nfeats, 
                 nchannels, 
                 bias=False, 
                 apply_sn=False, 
                 norm_type='selfmod', 
                 random_noise=False, 
                 use_style=False,
                 apply_pixel_norm=False):
        super(GenModel, self).__init__()
        d = nfeats * 8
        self.mapping = nn.Sequential(
            SN(nn.Linear(latent_dim, latent_dim, bias=bias)),
            nn.LeakyReLU(0.2)
        )
        self.linear = ProjectLayer(latent_dim, latent_dim, 8 * 8 * nfeats, [nfeats, 8, 8], bias, apply_sn, norm_type, random_noise, use_style, apply_pixel_norm)
        self.conv2 = UpConvLayer(latent_dim, nfeats, nfeats // 2, 4, 2, 1, bias, apply_sn, norm_type, random_noise, use_style, apply_pixel_norm)
        self.conv3 = UpConvLayer(latent_dim, nfeats // 2, nfeats // 4, 4, 2, 1, bias, apply_sn, norm_type, random_noise, use_style, apply_pixel_norm)
        self.conv4 = UpConvLayer(latent_dim, nfeats // 4, nfeats // 8, 4, 2, 1, bias, apply_sn, norm_type, random_noise, use_style, apply_pixel_norm)
        self.conv6 = nn.Conv2d(nfeats // 8, nchannels, 1, 1, 0, bias=bias)
        if apply_sn:
            self.conv6 = SN(self.conv6)
        
    def forward(self, x):
        latent_vec = self.mapping(x)
        out = self.linear(x, latent_vec)
        out = self.conv2(out, latent_vec)
        out = self.conv3(out, latent_vec)
        out = self.conv4(out, latent_vec)
        out = torch.tanh(self.conv6(out))
        return out

# Define the discriminator model
class DiscModel(nn.Module):
    def __init__(self, nchannels, nfeats):
        super(DiscModel, self).__init__()
        d = nfeats
        self.from_rgb = nn.Sequential(
            SN(nn.Conv2d(nchannels, d // 8, 1, 1, 0, bias=False)),
            nn.BatchNorm2d(d // 8),
            nn.LeakyReLU(0.2)
        )
        self.conv2 = nn.Sequential(
            SN(nn.Conv2d(d // 8, d // 8, 3, 1, 1, bias=False)),
            nn.BatchNorm2d(d // 8),
            nn.LeakyReLU(0.2),
            SN(nn.Conv2d(d // 8, d // 4, 4, 2, 1, bias=False)),
            nn.BatchNorm2d(d // 4),
            nn.LeakyReLU(0.2)
        )
        self.conv3 = nn.Sequential(
            SN(nn.Conv2d(d // 4, d // 4, 3, 1, 1, bias=False)),
            nn.BatchNorm2d(d // 4),
            nn.LeakyReLU(0.2),
            SN(nn.Conv2d(d // 4, d // 2, 4, 2, 1, bias=False)),
            nn.BatchNorm2d(d // 2),
            nn.LeakyReLU(0.2)
        )
        self.conv4 = nn.Sequential(
            SN(nn.Conv2d(d // 2, d // 2, 3, 1, 1, bias=False)),
            nn.BatchNorm2d(d // 2),
            nn.LeakyReLU(0.2),
            SN(nn.Conv2d(d // 2, d, 4, 2, 1, bias=False)),
            nn.BatchNorm2d(d),
            nn.LeakyReLU(0.2)
        )
        self.conv5 = nn.Sequential(
            SN(nn.Conv2d(d, d, 3, 1, 1, bias=False)),
            nn.BatchNorm2d(d),
            nn.LeakyReLU(0.2)
        )
        self.linear = SN(nn.Linear(8 * 8 * d, 1, bias=False))
        
    def forward(self, x):
        x = self.from_rgb(x)
        x = self.conv2(x)
        x = self.conv3(x)
        x = self.conv4(x)
        x = self.conv5(x)
        x = x.view(x.shape[0], -1)
        x = self.linear(x)
        return x

from numpy.random import choice
import random
from torch.autograd import Variable, grad

# Define gradient penalty function
def gradient_penalty(x, y, f):
    shape = [x.size(0)] + [1] * (x.dim() - 1)
    alpha = torch.rand(shape).to(x.device)
    z = x + alpha * (y - x)
    z = Variable(z, requires_grad=True).to(x.device)
    o = f(z)
    g = grad(o, z, grad_outputs=torch.ones(o.size()).to(x.device), create_graph=True)[0].view(z.size(0), -1)
    gp = ((g.norm(p=2, dim=1) - 1)**2).mean()
    return gp

# Define R1 penalty function
def R1Penalty(real_img, f):
    reals = Variable(real_img, requires_grad=True).to(real_img.device)
    real_logit = f(reals)
    apply_loss_scaling = lambda x: x * torch.exp(x * torch.Tensor([np.float32(np.log(2.0))]).to(real_img.device))
    undo_loss_scaling = lambda x: x * torch.exp(-x * torch.Tensor([np.float32(np.log(2.0))]).to(real_img.device))
    real_logit = apply_loss_scaling(torch.sum(real_logit))
    real_grads = grad(real_logit, reals, grad_outputs=torch.ones(real_logit.size()).to(reals.device), create_graph=True)[0].view(reals.size(0), -1)
    real_grads = undo_loss_scaling(real_grads)
    r1_penalty = torch.sum(torch.mul(real_grads, real_grads))
    return r1_penalty

# Define WGAN loss for the generator
def generator_wgan(G, D, latent_dim, batch_sz):
    noise = torch.randn(batch_sz, latent_dim, device=device)
    fake_images = G(noise)
    fake_logit = D(fake_images)
    G_loss = -fake_logit.mean()
    return G_loss

# Define WGAN-GP loss for the discriminator
def discriminator_wgan_gp(G, D, real_images, latent_dim, lammy=10.0, eps=0.001):
    batch_sz = real_images.shape[0]
    real_logit = D(real_images)
    noise = torch.randn(batch_sz, latent_dim, device=device)
    fake_images = G(noise)
    fake_logit = D(fake_images.detach())
    D_loss = fake_logit.mean() - real_logit.mean()
    D_loss += gradient_penalty(real_images.data, fake_images.data, D) * lammy
    return D_loss

# Define NS-GAN loss for the discriminator
def discriminator_NS(G, D, real_images, latent_dim, real_labels, fake_labels):
    batch_sz = real_images.shape[0]
    real_logit = D(real_images)
    D_loss_real = func.binary_cross_entropy_with_logits(real_logit, real_labels)
    noise = torch.randn(batch_sz, latent_dim, device=device)
    fake_images = G(noise)
    fake_logit = D(fake_images.detach())
    D_loss_fake = func.binary_cross_entropy_with_logits(fake_logit, fake_labels)
    D_loss = D_loss_real + D_loss_fake
    return D_loss, D_loss_real.item(), D_loss_fake.item()

# Define NS-GAN loss for the generator
def generator_NS(G, D, latent_dim, batch_sz, real_labels):
    noise = torch.randn(batch_sz, latent_dim, device=device)
    fake_images = G(noise)
    fake_logit = D(fake_images)
    G_loss = func.binary_cross_entropy_with_logits(fake_logit, real_labels)
    return G_loss

# Define Hinge loss for the discriminator
def discriminator_Hinge(G, D, real_images, latent_dim):
    batch_sz = real_images.shape[0]
    real_logit = D(real_images)
    D_loss_real = torch.mean(func.relu(1.0 - real_logit)) 
    noise = torch.randn(batch_sz, latent_dim, device=device)
    fake_images = G(noise)
    fake_logit = D(fake_images.detach())
    D_loss_fake = torch.mean(func.relu(1.0 + fake_logit)) 
    D_loss = D_loss_real + D_loss_fake
    return D_loss, D_loss_real, D_loss_fake

# Define Hinge loss for the generator
def generator_Hinge(G, D, latent_dim, batch_sz):
    noise = torch.randn(batch_sz, latent_dim, device=device)
    fake_images = G(noise)
    fake_logit = D(fake_images)
    G_loss = -torch.mean(fake_logit)
    return G_loss

# Define the training loop class
class TrainLoop:
    def __init__(self, latent_dim, G, D, r1_gamma=0.0, track_grads=False):
        self.latent_dim = latent_dim
        self.track_grads = track_grads
        self.G = G
        self.D = D
        self.fixed_noise = torch.randn(64, self.latent_dim)
        self.r1_gamma = r1_gamma
        
        self.d_losses = []
        self.g_losses = []
        self.d_losses_real = []
        self.d_losses_fake = []
        self.img_list = []
        self.g_grads = []
        self.d_grads = []

    def check_grads(self, model):
        grads = []
        for n, p in model.named_parameters():
            if not p.grad is None and p.requires_grad and "bias" not in n:
                grads.append(float(p.grad.abs().mean()))
        return grads

    def train(self, num_epochs, loader, criterion, optim_G, optim_D, scheduler_D, scheduler_G, print_intrvl, show_intrvl, plot_intrvl, loss='NS'):
        step = 0
        for epoch in tqdm(range(num_epochs)):
            for ii, real_images in enumerate(loader):
                batch_sz = real_images.size(0)
                if use_soft_labels:
                    real_labels = torch.empty((batch_sz, 1), device=device).uniform_(0.80, 0.95)
                    fake_labels = torch.empty((batch_sz, 1), device=device).uniform_(0.05, 0.20)
                else:
                    real_labels = torch.full((batch_sz, 1), 0.95, device=device)
                    fake_labels = torch.full((batch_sz, 1), 0.05, device=device)

                if invert_labels and random.random() < 0.01:
                    real_labels, fake_labels = fake_labels, real_labels

                self.D.zero_grad()
                real_images = real_images.to(device)

                if loss == 'WGAN':
                    D_loss = discriminator_wgan_gp(self.G, self.D, real_images, self.latent_dim)
                elif loss == 'HINGE':
                    D_loss, D_loss_real, D_loss_fake = discriminator_Hinge(self.G, self.D, real_images, self.latent_dim)
                else:
                    D_loss, D_loss_real, D_loss_fake = discriminator_NS(self.G, self.D, real_images, self.latent_dim, real_labels, fake_labels)

                D_loss.backward()
                optim_D.step()

                self.G.zero_grad()

                if loss == 'WGAN':
                    G_loss = generator_wgan(self.G, self.D, self.latent_dim, batch_sz)
                elif loss == 'HINGE':
                    G_loss = generator_Hinge(self.G, self.D, self.latent_dim, batch_sz)
                else:
                    G_loss = generator_NS(self.G, self.D, self.latent_dim, batch_sz, real_labels)
                G_loss.backward()
                optim_G.step()

                if step % print_intrvl == 0:
                    print('[%d/%d][%d/%d]\tLoss_D: %.4f\tLoss_G: %.4f\tD(x): %.4f\tD(G(z)): %.4f' 
                          % (epoch, num_epochs, ii, len(loader),
                             D_loss.item(), G_loss.item(), D_loss_real, D_loss_fake))

                if step % plot_intrvl == 0:
                    self.d_losses.append(D_loss.item())
                    self.g_losses.append(G_loss.item())
                    self.d_losses_real.append(D_loss_real)
                    self.d_losses_fake.append(D_loss_fake)
                    if self.track_grads:
                        self.g_grads.append(self.check_grads(self.G))
                        self.d_grads.append(self.check_grads(self.D))

                if (step % show_intrvl == 0) or ((epoch == num_epochs-1) and (ii == len(loader)-1)):
                    with torch.no_grad():
                        fake = self.G(self.fixed_noise.to(device)).detach().cpu()
                    self.img_list.append(make_grid(fake, padding=2, normalize=True))

                step += 1

# Initialize generator and discriminator models
generator = GenModel(latent_dim, feature_size, 3, False, spectral_norm, normalization_type, use_random_noise, apply_style, pixel_norm).to(device)
print(generator)
discriminator = DiscModel(3, feature_size).to(device)
print(discriminator)

# Initialize weights for the models
def init_weights(m):
    if type(m) == nn.Linear or type(m) == nn.Conv2d or type(m) == nn.ConvTranspose2d:
        torch.nn.init.kaiming_uniform(m.weight)
        if m.bias is not None:
            m.bias.data.zero_()

generator.apply(init_weights)
discriminator.apply(init_weights)

# Define loss criterion and optimizers
criterion = nn.BCELoss()
optimizerD = optim.Adam(discriminator.parameters(), lr=d_lr, betas=(beta1_val, beta2_val))
optimizerG = optim.Adam(generator.parameters(), lr=g_lr, betas=(beta1_val, beta2_val))
scheduler_D = optim.lr_scheduler.ExponentialLR(optimizerD, gamma=0.99)
scheduler_G = optim.lr_scheduler.ExponentialLR(optimizerG, gamma=0.99)

# Initialize and start training
trainer = TrainLoop(latent_dim, generator, discriminator, track_grads=True)
trainer.train(NUM_EPOCHS, data_loader, criterion, optimizerG, optimizerD, scheduler_D, scheduler_G, PRNT_INTRVL, SHOW_INTRVL, PLOT_INTRVL, loss=loss_type)

# Plot the generator and discriminator losses during training
plt.figure(figsize=(10, 5))
plt.title("Generator and Discriminator Loss During Training")
plt.plot(trainer.g_losses, label="G")
plt.plot(trainer.d_losses, label="D")
plt.xlabel("iterations")
plt.ylabel("Loss")
plt.legend()
plt.show()

# Plot D(x) and D(G(z)) during training
plt.figure(figsize=(10, 5))
plt.title("Generator and Discriminator Loss During Training")
plt.plot(trainer.d_losses_fake, label="D(G(z))")
plt.plot(trainer.d_losses_real, label="D(x)")
plt.xlabel("iterations")
plt.ylabel("Loss")
plt.legend()
plt.show()

# Check gradients for the generator
for i in trainer.g_grads: plt.plot(i)
plt.legend(range(5))

# Check gradients for the discriminator
for i in trainer.d_grads: plt.plot(i)
plt.legend(range(5))

# Display generated fake images
plt.figure(figsize=(15, 15))
plt.subplot(1, 2, 1)
plt.axis("off")
plt.title("Fake Images")
plt.imshow(np.transpose(trainer.img_list[-1], (1, 2, 0)))
plt.show()

# Save generated images to the output directory
if not os.path.exists(OUT_PATH):
    os.mkdir(OUT_PATH)
with torch.no_grad():
    for i_batch in range(0, num_images, 25):
        gen_z = torch.randn(25, latent_dim, device=device)
        gen_images = generator(gen_z)
        images = gen_images.to("cpu").clone().detach()
        images = images.numpy().transpose(0, 2, 3, 1)
        for i_image in range(gen_images.size(0)):
            save_image(gen_images[i_image, :, :, :] * 0.5 + 0.5, os.path.join(OUT_PATH, f'image_{i_batch+i_image:05d}.png'))

import shutil
shutil.make_archive('images', 'zip', OUT_PATH)
